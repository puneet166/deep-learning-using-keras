{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this, we will use the popular mnist dataset. This dataset consists of 70,000 images of handwritten digits from 0–9. We will attempt to identify them using a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The mnist dataset is conveniently provided to us as part of the Keras library, so we can easily load the dataset. Out of the 70,000 images provided in the dataset, 60,000 are given for training and 10,000 are given for testing.\n",
    "\n",
    "\n",
    "# When we load the dataset below, X_train and X_test will contain the images, and x_train_label and x_test_label will contain the digits that those images represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, X_train_label), (X_test, X_test_label) = mnist.load_data()\n",
    "#if you already have data set . here you can use spilt fun of sklearn for splite dataset into test and train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let’s take a look at one of the images in our dataset to see what we are working with. We will plot the first image in our dataset and check its size using the ‘shape’ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x153c6da4e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot the first image in the dataset\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the image shape ,it mean which type of image it is and whats pixel of this image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape\n",
    "# it show our image matrix 28*28(28 row and 28 column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images in our data set is 28*28 pixel images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By default, the shape of every image in the mnist dataset is 28 x 28, so we will not need to check the shape of all the images. When using real-world datasets, you may not be so lucky. 28 x 28 is also a fairly small size, so the CNN will be able to run over each image pretty quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we need to reshape our dataset inputs (X_train and X_test) to the shape that our model expects when we train the model. The first number is the number of images (60,000 for X_train and 10,000 for X_test). Then comes the shape of each image (28x28). The last number is 1, which signifies that the images are greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "               # @-why normalize ?\n",
    "               # ans-for fast processing and quick learn our CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @= why we required reshape of image?\n",
    "# ans= we already looked our image size of image that is 28*28, but we knew our model expect as input (size*size* which type of image) for processing\n",
    "# if image is grey(black & white ) we give 1(one channel).if our image is color full of RBG we give 3(channel ).\n",
    "# suppose our image 3*3 pixel - in grey our image size =(3*3)*1\n",
    "#in color image in RGB (3*3)*3\n",
    "# so , we need reshape all pixel of our input image pixel*pixel*channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential # its for implement nureal network\n",
    "from keras.layers import Dense, Conv2D, Flatten # we include Dense Layer and Conv2D Flatten\n",
    "#Dense = its fully connected layer in CNN (learn more about it)\n",
    "# Conv2D = its convolution 2D , you already know what convolution does (go to notes)\n",
    "# Flatten= its convert 2D array into 1D array(vector) and its usefull to connect convolution layer to dense layer(dense layer support one D array )\n",
    "# flatten is bridge between CNN layer and dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "# CNN creation same like ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 1st  layers into CNN\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "# 64- in first layer contains 64 filters (here we consider nureons as filters, but in ANN we call nureon)\n",
    "# kernal_zise(filter)= filters which this layer will use it would be 3*3 pixels\n",
    "#activations function relu\n",
    "# input_shape = 28*28*1 (28*28)represent size of image into pixel and 1 represent channel of image its grey image\n",
    "# if your accuracy will not be good then you can add here drop out,add more convolutions layer,maxpooling and padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its 2nd layer in CNN\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "# if your accuracy will not be good then you can add here drop out,maxpooling,add more convolutions layer and padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add more layer but here we use only 2 Convolutions layer\n",
    "model.add(Flatten())\n",
    "# now we adding flatten it convert 2 or 3 or 4 D array into single D vector for Dense layer becoze dense layer support one D vector\n",
    "# flatten act as bridge between convolution layer and dense layer \n",
    "# so that we add flatten\n",
    "\n",
    "#  now you can you drop out to check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its 3rd layer of CNN\n",
    "\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "# its dense layer its output layer with  softmax activation function with 10 nureon in output layer.\n",
    "# learn why use dense layer\n",
    "# @-why we add 10 nureons as ouput layer ?\n",
    "# ans= becoz our ouput would be 0 to 9 in this case , but if our output is classifer we use only one nurons with sigmoid activation function\n",
    "# learn about softmax\n",
    "# if your accuracy will not be good then you can add here fully connected layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model type that we will be using is Sequential. Sequential is the easiest way to build a model in Keras. It allows you to build a model layer by layer.\n",
    "# We use the ‘add()’ function to add layers to our model.\n",
    "# Our first 2 layers are Conv2D layers. These are convolution layers that will deal with our input images, which are seen as 2-dimensional matrices.\n",
    "# 64 in the first layer and 32 in the second layer are the number of nodes in each layer. This number can be adjusted to be higher or lower, depending on the size of the dataset. In our case, 64 and 32 work well, so we will stick with this for now.\n",
    "# Kernel size is the size of the filter matrix for our convolution. So a kernel size of 3 means we will have a 3x3 filter matrix. Refer back to the introduction and the first image for a refresher on this.\n",
    "# Activation is the activation function for the layer. The activation function we will be using for our first 2 layers is the ReLU, or Rectified Linear Activation. This activation function has been proven to work well in neural networks.\n",
    "# Our first layer also takes in an input shape. This is the shape of each input image, 28,28,1 as seen earlier on, with the 1 signifying that the images are greyscale.\n",
    "# In between the Conv2D layers and the dense layer, there is a ‘Flatten’ layer. Flatten serves as a connection between the convolution and dense layers.\n",
    "# ‘Dense’ is the layer type we will use in for our output layer. Dense is a standard layer type that is used in many cases for neural networks.\n",
    "# We will have 10 nodes in our output layer, one for each possible outcome (0–9).\n",
    "# The activation is ‘softmax’. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics.\n",
    "# The optimizer controls the learning rate. We will be using ‘adam’ as our optmizer. Adam is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training.\n",
    "# The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but the time it takes to compute the weights will be longer.\n",
    "# We will use ‘categorical_crossentropy’ for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "# To make things even easier to interpret, we will use the ‘accuracy’ metric to see the accuracy score on the validation set when we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "# its same ANN no explaination required\n",
    "# here use optimizer adam\n",
    "#loss fun learn about this loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #The loss function. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss. Keras distinguishes between binary_crossentropy (2 classes) and categorical_crossentropy (>2 classes), so we’ll use the latter. See all Keras losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will train our model. To train, we will use the ‘fit()’ function on our model with the following parameters: training data (train_X), target data (train_y), validation data, and the number of epochs.\n",
    "# For our validation data, we will use the test set provided to us in our dataset, which we have split into X_test and X_test_label.\n",
    "# The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch. For our model, we will set the number of epochs to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 394s 7ms/step - loss: 0.1284 - accuracy: 0.9619 - val_loss: 0.0529 - val_accuracy: 0.9823\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 418s 7ms/step - loss: 0.0494 - accuracy: 0.9842 - val_loss: 0.0524 - val_accuracy: 0.9832\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 405s 7ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 0.0466 - val_accuracy: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x153c2e9ce10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, X_train_label,batch_size=30, validation_data=(X_test, X_test_label), epochs=3)\n",
    "# its all same like ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for better accuracy you should be check of all permutations and combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 -Using our model to make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  If you want to see the actual predictions that our model has made for the test data, we can use the predict function. The predict function will give an array with 10 numbers. These numbers are the probabilities that the input image represents each digit (0–9). The array index with the highest number represents the model prediction. The sum of each array equals 1 (since each number is a probability).\n",
    "# To show this, we will show the predictions for the first 4 images in the test set.\n",
    "# Note: If we have new data, we can input our new data into the predict function to see the predictions our model makes on the new data. Since we don’t have any new unseen data, we will show predictions using the test set for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.3877701e-12, 1.9642664e-12, 1.8591868e-09, 1.5709962e-07,\n",
       "        9.6563492e-16, 5.2341315e-11, 2.2969012e-17, 9.9999988e-01,\n",
       "        1.1891376e-09, 7.7156157e-09],\n",
       "       [1.1871401e-09, 6.1737938e-07, 9.9999893e-01, 8.6051423e-11,\n",
       "        2.7841439e-14, 1.6276803e-13, 5.1277965e-07, 5.2620563e-14,\n",
       "        2.6706028e-08, 7.5646282e-13],\n",
       "       [2.9977696e-07, 9.9977523e-01, 5.0973991e-05, 3.9194529e-07,\n",
       "        1.8705701e-05, 7.3273814e-06, 2.5495612e-07, 6.8394693e-05,\n",
       "        7.7313089e-05, 1.1095869e-06],\n",
       "       [9.9989545e-01, 9.1832132e-11, 2.7130429e-06, 7.2472488e-07,\n",
       "        1.4873620e-10, 2.6140728e-07, 9.5606456e-06, 3.9913181e-07,\n",
       "        1.0918423e-07, 9.0791531e-05]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict first 4 images in the test set\n",
    "model.predict(X_test[:4])\n",
    "# its give probablity ,which probablity is higher in array , which no is predictable no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that our model predicted 7, 2, 1 and 0 for the first four images.\n",
    "# Let’s compare this with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual results for first 4 images in test set\n",
    "X_test_label[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The actual results show that the first four images are also 7, 2,1 and 0. Our model predicted correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
